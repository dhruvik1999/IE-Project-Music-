{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
    "\n",
    "#import all modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attributes of batch\n",
    "\n",
    "batchSize=16\n",
    "seqLength=64\n",
    "cij= \"char_to_index.json\"\n",
    "model_weights_directory = 'Model_Weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBatch(allChars,uniqueChars):\n",
    "    size=allChars.shape[0]\n",
    "    batchCapacity=int(size/batchSize)\n",
    "    \n",
    "    for s in range(0, batchCapacity- seqLength, 64):\n",
    "    \n",
    "        X = np.zeros((batchSize, seqLength))\n",
    "        Y = np.zeros((batchSize, seqLength, uniqueChars))\n",
    "        \n",
    "        for i in range(0,16):\n",
    "            for j in range(0,64):\n",
    "                X[i, j] = allChars[i * batchCapacity + s + j]\n",
    "                Y[i, j, allChars[i * batchCapacity + s + j + 1]] = 1 \n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_model(batch_size, seq_length,uniqueChars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = uniqueChars, output_dim = 512, batch_input_shape = (batch_size, seq_length), name = \"embd_1\")) \n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True, name = \"lstm_1\"))\n",
    "    model.add(Dropout(0.2, name = \"drp_1\"))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True,name = \"lstm_2\"))\n",
    "    model.add(Dropout(0.2,name = \"drp_2\"))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True,name = \"lstm_3\"))\n",
    "    model.add(Dropout(0.2,name = \"drp_3\"))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(uniqueChars)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    # TODO : load weights.\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, epochs = 40):\n",
    "    #mapping character to index number\n",
    "    cti = {ch: i for (i, ch) in enumerate(sorted(list(set(data))))}\n",
    "    print(\"Number of unique characters included in our ABC database = {}\".format(len(cti)))\n",
    "    #Will print the number of different charachters in our database\n",
    "    \n",
    "    with open(cij, mode = \"w\") as f:\n",
    "        json.dump(cti, f)\n",
    "        \n",
    "    itc = {i: ch for (ch, i) in cti.items()}\n",
    "    uniqueChars = len(cti)\n",
    "    \n",
    "    model = built_model(batchSize, seqLength, uniqueChars)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    allChar = np.asarray([cti[c] for c in data], dtype = np.int32)\n",
    "    print(\"Total number of characters = \"+str(allChar.shape[0])) #155222\n",
    "    \n",
    "    epoch_number, loss, accuracy = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
    "        epoch_number.append(epoch+1)\n",
    "        \n",
    "        for i, (x, y) in enumerate(makeBatch(allChar, uniqueChars)):\n",
    "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y) \n",
    "            #Training On Batch\n",
    "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_accuracy))\n",
    "            #here, above we are reading the batches one-by-one and train our model on each batch one-by-one.\n",
    "        loss.append(final_epoch_loss)\n",
    "        accuracy.append(final_epoch_accuracy)\n",
    "        \n",
    "        #saving weights after every 10 epochs\n",
    "        #Weights to be saved 4 times\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            if not os.path.exists(model_weights_directory):\n",
    "                os.makedirs(model_weights_directory)\n",
    "            model.save_weights(os.path.join(model_weights_directory, \"Weights_{}.h5\".format(epoch+1)))\n",
    "            print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1, epoch+1))\n",
    "    \n",
    "    #creating dataframe and record all the losses and accuracies at each epoch Total 40\n",
    "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "    log_frame[\"Epoch\"] = epoch_number\n",
    "    log_frame[\"Loss\"] = loss\n",
    "    log_frame[\"Accuracy\"] = accuracy\n",
    "    log_frame.to_csv(\"log.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters included in our ABC database = 82\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embd_1 (Embedding)           (16, 64, 512)             41984     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (16, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "drp_1 (Dropout)              (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "drp_2 (Dropout)              (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "drp_3 (Dropout)              (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (16, 64, 82)              21074     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (16, 64, 82)              0         \n",
      "=================================================================\n",
      "Total params: 1,901,138\n",
      "Trainable params: 1,901,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total number of characters = 33719\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruvik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Loss: 4.420258522033691, Accuracy: 0.005859375\n",
      "Batch: 2, Loss: 4.105154991149902, Accuracy: 0.1240234375\n",
      "Batch: 3, Loss: 3.930936336517334, Accuracy: 0.0986328125\n",
      "Batch: 4, Loss: 3.596402406692505, Accuracy: 0.1220703125\n",
      "Batch: 5, Loss: 3.327986001968384, Accuracy: 0.1259765625\n",
      "Batch: 6, Loss: 3.503667116165161, Accuracy: 0.1240234375\n",
      "Batch: 7, Loss: 3.5223934650421143, Accuracy: 0.142578125\n",
      "Batch: 8, Loss: 3.4347286224365234, Accuracy: 0.13671875\n",
      "Batch: 9, Loss: 3.630373239517212, Accuracy: 0.115234375\n",
      "Batch: 10, Loss: 3.937164306640625, Accuracy: 0.0576171875\n",
      "Batch: 11, Loss: 3.478513240814209, Accuracy: 0.091796875\n",
      "Batch: 12, Loss: 3.3535823822021484, Accuracy: 0.1123046875\n",
      "Batch: 13, Loss: 3.374765157699585, Accuracy: 0.1142578125\n",
      "Batch: 14, Loss: 3.4170644283294678, Accuracy: 0.1298828125\n",
      "Batch: 15, Loss: 3.323763608932495, Accuracy: 0.1318359375\n",
      "Batch: 16, Loss: 3.5368857383728027, Accuracy: 0.1318359375\n",
      "Batch: 17, Loss: 3.4910178184509277, Accuracy: 0.119140625\n",
      "Batch: 18, Loss: 3.230708122253418, Accuracy: 0.1708984375\n",
      "Batch: 19, Loss: 3.268427848815918, Accuracy: 0.162109375\n",
      "Batch: 20, Loss: 3.4182991981506348, Accuracy: 0.1689453125\n",
      "Batch: 21, Loss: 3.2800683975219727, Accuracy: 0.1533203125\n",
      "Batch: 22, Loss: 3.2199058532714844, Accuracy: 0.1640625\n",
      "Batch: 23, Loss: 3.6566550731658936, Accuracy: 0.1220703125\n",
      "Batch: 24, Loss: 3.5870203971862793, Accuracy: 0.119140625\n",
      "Batch: 25, Loss: 3.4108057022094727, Accuracy: 0.140625\n",
      "Batch: 26, Loss: 3.4353928565979004, Accuracy: 0.1455078125\n",
      "Batch: 27, Loss: 3.1890101432800293, Accuracy: 0.1845703125\n",
      "Batch: 28, Loss: 3.3007967472076416, Accuracy: 0.1494140625\n",
      "Batch: 29, Loss: 3.3230128288269043, Accuracy: 0.154296875\n",
      "Batch: 30, Loss: 3.568463087081909, Accuracy: 0.119140625\n",
      "Batch: 31, Loss: 3.4469478130340576, Accuracy: 0.150390625\n",
      "Batch: 32, Loss: 3.0912070274353027, Accuracy: 0.1708984375\n",
      "Saved Weights at epoch 1 to file Weights_1.h5\n",
      "Epoch 2/40\n",
      "Batch: 1, Loss: 3.625504732131958, Accuracy: 0.115234375\n",
      "Batch: 2, Loss: 3.331554412841797, Accuracy: 0.14453125\n",
      "Batch: 3, Loss: 3.526991367340088, Accuracy: 0.103515625\n",
      "Batch: 4, Loss: 3.3247482776641846, Accuracy: 0.142578125\n",
      "Batch: 5, Loss: 3.099440097808838, Accuracy: 0.1552734375\n",
      "Batch: 6, Loss: 3.322336196899414, Accuracy: 0.146484375\n",
      "Batch: 7, Loss: 3.361743927001953, Accuracy: 0.126953125\n",
      "Batch: 8, Loss: 3.307267665863037, Accuracy: 0.1171875\n",
      "Batch: 9, Loss: 3.519848346710205, Accuracy: 0.111328125\n",
      "Batch: 10, Loss: 3.829483985900879, Accuracy: 0.07421875\n",
      "Batch: 11, Loss: 3.4108195304870605, Accuracy: 0.126953125\n",
      "Batch: 12, Loss: 3.253068208694458, Accuracy: 0.1494140625\n",
      "Batch: 13, Loss: 3.322216749191284, Accuracy: 0.12109375\n",
      "Batch: 14, Loss: 3.3584742546081543, Accuracy: 0.15234375\n",
      "Batch: 15, Loss: 3.2584104537963867, Accuracy: 0.1513671875\n",
      "Batch: 16, Loss: 3.464056968688965, Accuracy: 0.13671875\n",
      "Batch: 17, Loss: 3.436452865600586, Accuracy: 0.1162109375\n",
      "Batch: 18, Loss: 3.2083468437194824, Accuracy: 0.1787109375\n",
      "Batch: 19, Loss: 3.2396655082702637, Accuracy: 0.162109375\n",
      "Batch: 20, Loss: 3.369415760040283, Accuracy: 0.169921875\n",
      "Batch: 21, Loss: 3.2524542808532715, Accuracy: 0.1669921875\n",
      "Batch: 22, Loss: 3.18318510055542, Accuracy: 0.1640625\n",
      "Batch: 23, Loss: 3.60573410987854, Accuracy: 0.134765625\n",
      "Batch: 24, Loss: 3.5560874938964844, Accuracy: 0.1259765625\n",
      "Batch: 25, Loss: 3.3755319118499756, Accuracy: 0.1513671875\n",
      "Batch: 26, Loss: 3.4035661220550537, Accuracy: 0.1669921875\n",
      "Batch: 27, Loss: 3.1452479362487793, Accuracy: 0.20703125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-df232fef3c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-de060a0f0ae6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(data, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmakeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallChar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniqueChars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mfinal_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_epoch_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m#Training On Batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch: {}, Loss: {}, Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_epoch_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Datafile=\"Morris and Waltzes.txt\"\n",
    "file=open(Datafile,mode='r')\n",
    "data=file.read()\n",
    "file.close()\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"log.csv\")\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
